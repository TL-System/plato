clients:
    # The total number of clients
    total_clients: 100

    # The number of clients selected in each round
    per_round: 10

    # Should the clients compute test accuracy locally?
    do_test: false

server:
    address: 127.0.0.1
    port: 8001
    ping_timeout: 36000
    ping_interval: 36000
    simulate_wall_time: true

    # The paths for storing temporary checkpoints and models
    checkpoint_path: models/emnist
    model_path: models/emnist

data:
    # The training and testing dataset
    datasource: EMNIST

    # Number of samples in each partition
    partition_size: 1128

    # IID or non-IID?
    sampler: noniid

    # The concentration parameter for the Dirichlet distribution
    concentration: 5

    # Where the dataset is located
    data_path: data/EMNIST

trainer:
    # The maximum number of training rounds
    rounds: 120

    # The maximum number of clients running concurrently
    max_concurrency: 5

    # The target accuracy
    target_accuracy: 0.95

    # Number of epoches for local training in each communication round
    epochs: 5
    batch_size: 32
    optimizer: SGD

    # The machine learning model
    model_name: dlg
    num_classes: 47

algorithm:
    # Aggregation algorithm
    type: fedavg

    ## Data reconstruction related configurations
    # Choose from DLG (default), iDLG, csDLG
    attack_method: no
    # For calculating the loss between dummy and target gradients/updates
    # Choose from l2, l1, max, sim, simlocal
    cost_fn: l2
    # Weight for adding total data variation to the loss
    total_variation: 0
    # Learning rate of the optimizer to reconstruct data
    lr: 1

    # in a particular communication round (starting from 1)
    attack_round: 1
    # One particular client, e.g., the first selected client
    victim_client: 0
    num_iters: 3000
    log_interval: 300

    # Whether or not use customized initialization for model weights
    init_params: true
    # Whether sharing gradients or model weights
    share_gradients: true
    # Do reconstruction by directly matching weights when it's true,
    # otherwise by matching gradients calculated from updates
    match_weights: true
    # Matching weights (absolute) or weight updates (delta)
    use_updates: false
    # The number of times the attack will be attempted
    trials: 1

    ## Defense related
    # Choose from GradDefense, Soteria, GC (model compression), DP (differential privacy), Outpost (ours)...
    defense: no
    # for Outpost
    beta: 0.125 # controls iteration decay speed
    phi: 40 # the amount (%) of perturbation
    prune_base: 80 # for pruning
    noise_base: 0.8 # controls scale for gaussian noise
    # for GradDefense
    clip: true
    slices_num: 10
    perturb_slices_num: 5
    scale: 0.01
    Q: 6
    # for Soteria
    threshold: 50
    # for GC
    prune_pct: 80
    # for DP
    epsilon: 0.1

parameters:
    optimizer:
        lr: 0.01

results:
    result_path: results/emnist

    # Write the following parameter(s) into a CSV
    types: round, accuracy, elapsed_time, round_time

    # Plot results (x_axis-y_axis)
    plot: round-accuracy, elapsed_time-accuracy
