general:
    base_path: /data/ningxin/infocom

clients:
    # Type
    type: simple

    # The total number of clients
    total_clients: 70

    # The number of clients selected in each round
    per_round: 50

    # Should the clients compute test accuracy locally?
    do_test: false

    # Whether simulate clients or not
    speed_simulation: true

    # The simulation distribution
    simulation_distribution:
        distribution: pareto
        alpha: 1

    max_sleep_time: 80
    sleep_simulation: true
    avg_training_time: 20

    compute_comm_time: true

    random_seed: 1

    #------------------- Need modification -------------------
    # The round to remove the local data by clients
    # If do_optimized_clustering is true, it needs to add 2.
    # For example, if you want to do the data deletion after round 2
    # You should set it as round 4.
    data_deletion_round: 3

    # The clients which need to delete their local data samples
    clients_requesting_deletion: [1,2,3,4]

    # The percentage to delete the local data by clients
    deleted_data_ratio: 1

server:
    address: 127.0.0.1
    port: 8040

    simulate_wall_time: true

    checkpoint_path: checkpoints/shake/fedavg
    model_path: models/shake/fedavg

    random_seed: 1

    # Should we operate in sychronous mode?
    synchronous: false

    # Should we compute the test accuracy of the global model?
    do_test: true

    staleness_bound: 1000

    #------------------- Need modification -------------------

    # The minimum arrival clients that all clusters can aggregrate
    minimum_clients_aggregated: 25

#=================== Only for clustering =================

# The window size that accuracy of each cluster should reach
#window_size: 3

#optimization clustering method; need to be comment when run fedun.py
#do_optimized_clustering: false

# The total number of clusters; need to be comment when run fedun.py
# clusters: 2

data:
    data_path: data/HuggingFace

    # The training and testing dataset
    datasource: HuggingFace
    dataset_name: tiny_shakespeare

    # Number of samples in each partition
    partition_size: 500

    # IID or non-IID?
    sampler: iid

    # The random seed for sampling data
    random_seed: 1

trainer:
    # The type of the trainer
    type: HuggingFace

    # The maximum number of training rounds
    rounds: 25

    # The maximum number of clients running concurrently
    max_concurrency: 1

    # The target perplexity
    target_perplexity: 36

    # Number of epoches for local training in each communication round
    epochs: 5
    batch_size: 32
    optimizer: SGD
    learning_rate: 0.01
    momentum: 0.9
    weight_decay: 0.0

    # The machine learning model
    model_name: HuggingFace_CausalLM
    model_checkpoint: distilgpt2

    #=================== Only for clustering =================
    # The accuracy variance threshold that trigger the stop training sign
    #target_accuracy_std: 0.02
    target_perplexity_std: 0.1

algorithm:
    # Aggregation algorithm
    type: fedavg

results:
    result_path: results/hugging/fedun
    types: round, accuracy, elapsed_time
