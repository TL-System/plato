# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import torch

def alpha_divergence(q_logits, p_logits, alpha, reduction='none', iw_clip=1e3):
    assert isinstance(alpha, float)
    q_prob = torch.nn.functional.softmax(q_logits, dim=1)
    p_prob = torch.nn.functional.softmax(p_logits, dim=1)

    if abs(alpha) < 1e-3:
        lndiff = q_prob.log() - p_prob.log()
        lndiff.clamp_(-iw_clip, iw_clip)
        loss = torch.sum(q_prob * lndiff, dim=1) # KL(q||p)
    elif abs(alpha - 1.0) < 1e-3:
        loss = torch.sum(p_prob * (p_prob.log() - q_prob.log()), dim=1) # KL(p||q)
    else:
        iw_ratio = torch.pow(p_prob/q_prob, alpha)
        iw_ratio = iw_ratio.clamp(0, iw_clip)
        loss = 1.0 / (alpha*(alpha-1.0)) * ((iw_ratio * q_prob).sum(1) - 1.0) # D_a(p||q)

    if reduction == 'mean':
        return loss.mean()
    elif reduction == 'sum':
        return loss.sum()
    return loss

def f_min_divergence(q_logits, p_logits, alpha_left, alpha_right, iw_clip=1e3, p_normalize=False):
    q_prob = torch.nn.functional.softmax(q_logits, dim=1).detach()
    if p_normalize:
        p_prob = p_logits.detach()
    else:
        p_prob = torch.nn.functional.softmax(p_logits, dim=1).detach()

    q_log_prob = torch.nn.functional.log_softmax(q_logits, dim=1)
    importance_ratio = p_prob / q_prob

    iw_alpha_left = torch.pow(importance_ratio, alpha_left)
    iw_alpha_left = iw_alpha_left.clamp(0, iw_clip)
    f_left = iw_alpha_left / alpha_left / (alpha_left - 1.0)
    f_base_left = 1.0 / alpha_left / (alpha_left - 1.0)
    rho_f_left = iw_alpha_left / alpha_left + f_base_left

    iw_alpha_right = torch.pow(importance_ratio, alpha_right)
    iw_alpha_right = iw_alpha_right.clamp(0, iw_clip)
    f_right = iw_alpha_left / alpha_right / (alpha_right - 1.0)
    f_base_right = 1.0 / alpha_right / (alpha_right - 1.0)
    rho_f_right = iw_alpha_right / alpha_right + f_base_right

    ind = torch.gt(iw_alpha_left, iw_alpha_right)

    loss = torch.sum(q_prob * ((f_left - f_base_left) * ind + (f_right - f_base_right) * (1 - ind)), dim=1)
    grad_loss = -torch.sum(q_prob * rho_f * q_log_prob, dim=1)
    return loss, grad_loss

def f_divergence(q_logits, p_logits, alpha, iw_clip=1e3, p_normalize=False):
    assert isinstance(alpha, float)
    q_prob = torch.nn.functional.softmax(q_logits, dim=1).detach()
    if p_normalize:
        p_prob = p_logits.detach()
    else:
        p_prob = torch.nn.functional.softmax(p_logits, dim=1).detach()
    '''
    # smooth p_prob
    local_label_smooth = .1
    p_prob = p_prob + torch.ones_like(p_prob) * local_label_smooth / 1000.
    p_prob /= (local_label_smooth + 1)
    '''
    q_log_prob = torch.nn.functional.log_softmax(q_logits, dim=1) #gradient is only backpropagated here

    importance_ratio = p_prob / q_prob
    if abs(alpha) < 1e-3:
        importance_ratio = importance_ratio.clamp(0, iw_clip)
        f = -importance_ratio.log()
        f_base = 0
        rho_f = importance_ratio.log() - 1.0
    elif abs(alpha - 1.0) < 1e-3:
        f = importance_ratio * importance_ratio.log()
        f_base = 0
        rho_f = importance_ratio
    else:
        iw_alpha = torch.pow(importance_ratio, alpha)
        iw_alpha = iw_alpha.clamp(0., iw_clip)
        f = iw_alpha / alpha / (alpha - 1.0)
        f_base = 1.0 / alpha / (alpha - 1.0)
        rho_f = iw_alpha / alpha + f_base

    loss = torch.sum(q_prob * (f - f_base), dim=1)
    grad_loss = -torch.sum(q_prob * rho_f * q_log_prob, dim=1)
    return loss, grad_loss


class CrossEntropyLossSoft(torch.nn.modules.loss._Loss):
    """ inplace distillation for image classification """
    def forward(self, output, target):
        output_log_prob = torch.nn.functional.log_softmax(output, dim=1)
        target = target.unsqueeze(1)
        output_log_prob = output_log_prob.unsqueeze(2)
        cross_entropy_loss = -torch.bmm(target, output_log_prob)
        return cross_entropy_loss.mean()


class FdivTopKLossSoft(torch.nn.modules.loss._Loss):
    """ inplace distillation for image classification
            output: output logits of the student network
            target: output logits of the teacher network
    """
    def forward(self, output, target, T=1.0):
        output, target = output/T, target/T
        output_prob = torch.nn.functional.softmax(output, dim=1)
        output_log_prob = torch.nn.functional.log_softmax(output, dim=1)

        target_prob = torch.nn.functional.softmax(target, dim=1)
        #density ratios
        density_ratio = target_prob / output_prob
        #_, indices = torch.topk(density_ratio, k, dim=1, largest=True)
        #one_hot_w = torch.zeros_like(target).scatter(1, indices, 1)
        one_hot_w = torch.ge(density_ratio, 1.).float()

        ##probablity
        #_, indices = torch.topk(target_prob, k, dim=1, largest=True)
        #one_hot_p = torch.zeros_like(target).scatter(1, indices, 1)

        #one_hot = one_hot_w * one_hot_p
        one_hot = one_hot_w.detach()
        #loss = -torch.sum( target_prob * one_hot * output_log_prob, dim=1)
        loss = -torch.sum(target_prob.detach() * one_hot * output_log_prob, dim=1)

        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        return loss


class HardThresLossSoft(torch.nn.modules.loss._Loss):
    """ inplace distillation for image classification
            output: output logits of the student network
            target: output logits of the teacher network
    """
    def forward(self, output, target, eps=0.1):
        output_prob = torch.nn.functional.softmax(output, dim=1)
        output_log_prob = torch.nn.functional.log_softmax(output, dim=1)

        target_prob = torch.nn.functional.softmax(target, dim=1)
        one_hot = torch.ge(target_prob, eps).float()
        n_class = output.size(1)
        noise_labels = torch.sum(target_prob * (1. - one_hot), 1, keepdim=True) / (n_class - torch.sum(one_hot, 1, keepdim=True))
        target_prob = one_hot * target_prob + (1. - one_hot) * noise_labels

        loss = -torch.sum(target_prob * output_log_prob, dim=1)
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        return loss


class TopkLossSoft(torch.nn.modules.loss._Loss):
    """ inplace distillation for image classification
            output: output logits of the student network
            target: output logits of the teacher network
    """
    def forward(self, output, target, k=5):
        output_log_prob = torch.nn.functional.log_softmax(output, dim=1)

        target_prob = torch.nn.functional.softmax(target, dim=1)
        topk_vals, topk_idxs = torch.topk(target_prob, k, dim=1, largest=True)
        one_hot = torch.zeros_like(target).scatter(1, topk_idxs, 1) # topk, one hot
        n_class = output.size(1)
        noise_labels = torch.sum(target_prob * (1. - one_hot), 1, keepdim=True) / (n_class - k)
        target_prob = one_hot * target_prob + (1. - one_hot) * noise_labels

        loss = -torch.sum(target_prob * output_log_prob, dim=1)
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        return loss


#class AdaptiveLossSoft(torch.nn.modules.loss._Loss):
#    """ inplace distillation for image classification
#            output: output logits of the student network
#            target: output logits of the teacher network
#            grad_theta = -E_q[ rho_f(p/q) grad_theta \log q]
#    """
#    def forward(self, output, target, eps=1e-5, scale=.1):
#        output_prob = torch.nn.functional.softmax(output, dim=1)
#        target_prob = torch.nn.functional.softmax(target, dim=1)
#        output_log_prob = torch.nn.functional.log_softmax(output, dim=1)
#        target_log_prob = torch.nn.functional.log_softmax(target, dim=1)
#
#        rho_f = target_log_prob - output_log_prob - 1
#        rho_var = torch.var(rho_f, dim=1, keepdim=True)
#        rho_f = rho_f / torch.sqrt(rho_var + eps) * scale
#
#        loss = -torch.sum(output_prob.detach() * rho_f.detach() * output_log_prob, dim=1)
#        if self.reduction == 'mean':
#            return loss.mean()
#        elif self.reduction == 'sum':
#            return loss.sum()
#        return loss
#


class KLLossSoft(torch.nn.modules.loss._Loss):
    """ inplace distillation for image classification
            output: output logits of the student network
            target: output logits of the teacher network
            T: temperature
            KL(p||q) = Ep \log p - \Ep log q
    """
    def forward(self, output, soft_logits, target=None, temperature=1., alpha=0.9):
        output, soft_logits = output / temperature, soft_logits / temperature
        soft_target_prob = torch.nn.functional.softmax(soft_logits, dim=1)
        output_log_prob = torch.nn.functional.log_softmax(output, dim=1)
        kd_loss = -torch.sum(soft_target_prob * output_log_prob, dim=1)
        if target is not None:
            n_class = output.size(1)
            target = torch.zeros_like(output).scatter(1, target.view(-1, 1), 1)
            target = target.unsqueeze(1)
            output_log_prob = output_log_prob.unsqueeze(2)
            ce_loss = -torch.bmm(target, output_log_prob).squeeze()
            loss = alpha*temperature* temperature*kd_loss + (1.0-alpha)*ce_loss
        else:
            loss = kd_loss

        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        return loss


class ReverseKLLossSoft(torch.nn.modules.loss._Loss):
    """ inplace distillation for image classification
            output: output logits of the student network
            target: output logits of the teacher network
            T: temperature
            KL(q||p) = Eq(\log q - \log p)
    """
    def forward(self, output, target, T=1.):
        output, target = output/T, target/T
        output_prob = torch.nn.functional.softmax(output, dim=1)
        target_log_prob = torch.nn.functional.log_softmax(target, dim=1)

        loss = torch.sum(output_prob * (output_prob.log() - target_log_prob), dim=1)
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        return loss


class AdaptiveLossSoft(torch.nn.modules.loss._Loss):
    def __init__(self, alpha_min, alpha_max, iw_clip=5):
        super(AdaptiveLossSoft, self).__init__()
        self.alpha_min = alpha_min
        self.alpha_max = alpha_max
        self.iw_clip = iw_clip

    def forward(self, output, target, alpha_min=None, alpha_max=None, mode_seeking_weight=None, p_normalize=False, reduction=True):
        alpha_min = alpha_min or self.alpha_min
        alpha_max = alpha_max or self.alpha_max

        #loss_left = alpha_divergence(output, target, alpha_min, iw_clip=self.iw_clip)
        #loss_right = alpha_divergence(output, target, alpha_max, iw_clip=self.iw_clip)
        #loss = torch.max(loss_left, loss_right)
        if mode_seeking_weight is None:
            loss_left, grad_loss_left = f_divergence(output, target, alpha_min, iw_clip=self.iw_clip, p_normalize=p_normalize)
            loss_right, grad_loss_right = f_divergence(output, target, alpha_max, iw_clip=self.iw_clip, p_normalize=p_normalize)

            # change max -> min
            ind = torch.gt(loss_left, loss_right).float()
            loss = ind * grad_loss_left + (1.0 - ind) * grad_loss_right

        else:
            alpha = alpha_min * mode_seeking_weight + alpha_max * (1.0 - mode_seeking_weight)
            _, loss = f_divergence(output, target, alpha, iw_clip=self.iw_clip)
            #loss = mode_seeking_weight * grad_loss_left + (1.0 - mode_seeking_weight) * grad_loss_right

        if not reduction:
            return loss

        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        return loss


    #def forward(self, output, target, interpolation=1.0):
    #    loss_left = alpha_divergence(output, target, self.alpha_min)
    #    loss_right = alpha_divergence(output, target, self.alpha_max)
    #
    #    loss_min = torch.min(loss_left, loss_right)
    #    loss_max = torch.max(loss_left, loss_right)
    #    loss = (1.0 - interpolation)*loss_min + interpolation*loss_max
    #
    #    if self.reduction == 'mean':
    #        return loss.mean()
    #    elif self.reduction == 'sum':
    #        return loss.sum()
    #    return loss


class AlphaDivergenceLossSoft(torch.nn.modules.loss._Loss):
    """ alpha divergence
            output: output logits of the student network
            target: output logits of the teacher network
            T: temperature
            D_a(q||p) = 1/(a(a-1)) * E_q[p^a q^-a -1] = 1/(a(a-1)) (\sum p^a q^1-a - 1)
    """
    def forward(self, output, target, alpha):
        loss = alpha_divergence(output, target, alpha, reduction='none')
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        return loss


class EntropyAlphaDivergence(torch.nn.modules.loss._Loss):

    def forward(self, output, target):
        prob = torch.nn.functional.softmax(target, dim=1)
        ent = -torch.sum(prob * prob.log(), dim=1)
        #alpha = (ent - torch.min(ent)) / (torch.max(ent) - torch.min(ent))
        alpha = 1.0 - torch.max(prob, 1).values
        #alpha = torch.max(prob, 1).values
        loss = alpha_divergence(output, target, alpha, reduction='none')

        thr = torch.gt(torch.max(prob, 1).values, 0.8).float()
        forward_kl = alpha_divergence(output, target, 0.0, reduction='none')
        reverse_kl = alpha_divergence(output, target, 1.0, reduction='none')
        loss =  thr*forward_kl + (1.0 - thr)*reverse_kl

        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        return loss


'''
    idea: cross entropy loss doesn't satisfy triangle inequality
          we might want to use a symmetric divergence
'''
class JSDLossSoft(torch.nn.modules.loss._Loss):
    def __init__(self):
        super(JSDLossSoft, self).__init__()

    #{{\rm {JSD}}}(P\parallel Q)={\frac  {1}{2}}D(P\parallel M)+{\frac  {1}{2}}D(Q\parallel M)
    def forward(self, output, target):
        output_prob = torch.nn.functional.softmax(output, dim=1)
        target_prob = torch.nn.functional.softmax(target, dim=1)

        M = (output_prob + target_prob) / 2.
        # student network
        kl_qm = output_prob * (output_prob.log() - M.log())
        kl_pm = target_prob * (target_prob.log() - M.log())
        loss = torch.sum(0.5 * (kl_qm + kl_pm), dim=1)

        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        return loss


class JSDLossSmooth(torch.nn.modules.loss._Loss):
    def __init__(self, label_smoothing=0.1):
        super(JSDLossSmooth, self).__init__()
        self.eps = label_smoothing

    #{{\rm {JSD}}}(P\parallel Q)={\frac  {1}{2}}D(P\parallel M)+{\frac  {1}{2}}D(Q\parallel M)
    def forward(self, output, target):
        output_prob = torch.nn.functional.softmax(output, dim=1)
        n_class = output.size(1)
        one_hot = torch.zeros_like(output).scatter(1, target.view(-1, 1), 1)
        target_prob = one_hot * (1 - self.eps) + self.eps / n_class

        M = (output_prob + target_prob) / 2.
        # student network
        kl_qm = output_prob * (output_prob.log() - M.log())
        kl_pm = target_prob * (target_prob.log() - M.log())
        loss = torch.sum(0.5 * (kl_qm + kl_pm), dim=1)

        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        return loss


class CrossEntropyLossSmooth(torch.nn.modules.loss._Loss):
    def __init__(self, label_smoothing=0.1):
        super(CrossEntropyLossSmooth, self).__init__()
        self.eps = label_smoothing

    """ label smooth """
    def forward(self, output, target, reduction=True):
        n_class = output.size(1)
        one_hot = torch.zeros_like(output).scatter(1, target.view(-1, 1), 1)
        target = one_hot * (1 - self.eps) +  self.eps / n_class
        output_log_prob = torch.nn.functional.log_softmax(output, dim=1)
        target = target.unsqueeze(1)
        output_log_prob = output_log_prob.unsqueeze(2)
        loss = -torch.bmm(target, output_log_prob)

        if not reduction:
            return loss
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        return loss



class CrossEntropyEma(torch.nn.modules.loss._Loss):
    def __init__(self, label_smoothing=0.1):
        super(CrossEntropyEma, self).__init__()
        self.eps = label_smoothing

    def _forward(self, output, target):
        output_log_prob = torch.nn.functional.log_softmax(output, dim=1)
        target = target.unsqueeze(1)
        output_log_prob = output_log_prob.unsqueeze(2)
        loss = -torch.bmm(target, output_log_prob)
        return loss.squeeze(-2).squeeze(-1)

    def forward(self, output, target, ema_output=None, beta=0.1):
        n_class = output.size(1)
        one_hot = torch.zeros_like(output).scatter(1, target.view(-1, 1), 1)
        target = one_hot * (1 - self.eps) +  self.eps / n_class

        loss_model = self._forward(output, target)
        if ema_output is not None:
            loss_ema = self._forward(ema_output, target)
            loss_model_ema = self._forward(output, torch.nn.functional.softmax(ema_output, dim=1))
            indicators = torch.ge(loss_model, loss_ema).float() * beta
            loss = (1.0-indicators)*loss_model + indicators*loss_model_ema
        else:
            loss = loss_model
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        return loss


if __name__ == '__main__':
    output, target = torch.rand(4, 100), torch.rand(4, 100)
    print(output.size(), target.size())
    print(alpha_divergence(output, target, alpha=-0.1))
    print(f_divergence(output, target, alpha=-0.1))
